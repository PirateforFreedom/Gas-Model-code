import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import pandas as pd
import os
import string
#定义常量
rnn_unit=10       #hidden layer units
input_size=2      
output_size=1
lr=0.0006         #学习率
#def extract_gasload(gasload,gasloadlabel):
#    Tload=[]
#    Tload.clear()
#    gasindex=0    
#    for gas in gasload:
#        indexgas=gas.find('\t',0)
#        gasloadlabel.append(gas[0:indexgas])
#        Tload.append(gas[indexgas+1:len(gas)])
#    gasload.clear()
#    gasindex=0
#    while gasindex<len(gasloadlabel):
#         if gasindex==0:
#              tempload="0"
#         else:
#              tempload=gasloadlabel[gasindex-1]
#          #print(tempload)
#         gasload.append([Tload[gasindex],tempload])
#         gasindex=gasindex+1
#    return gasload,gasloadlabel
def get_train_data(data,batch_size=60,time_step=20,train_begin=0,train_end=2923):
    batch_index=[]
    data_train=data[train_begin:train_end]
    normalized_train_data=(data_train-np.mean(data_train,axis=0))/np.std(data_train,axis=0)  #标准化z-score 标准化
    train_x,train_y=[],[]   #训练集x和y初定义
    for i in range(len(normalized_train_data)-time_step):
       if i % batch_size==0:
           batch_index.append(i)
       x=normalized_train_data[i:i+time_step,:6]
       y=normalized_train_data[i:i+time_step,6,np.newaxis]
       train_x.append(x.tolist())
       train_y.append(y.tolist())
    batch_index.append((len(normalized_train_data)-time_step))
    return batch_index,train_x,train_y
def get_test_data(data,time_step=20,test_begin=2923):
    data_test=data[test_begin:]
    train_data=data[0:2923]
    mean=np.mean(train_data,axis=0)
    std=np.std(train_data,axis=0)
    normalized_test_data=(data_test-mean)/std  #标准化
    size=(len(normalized_test_data)+time_step)//time_step  #有size个sample 
    test_x,test_y=[],[]  
    for i in range(size-1):
       x=normalized_test_data[i*time_step:(i+1)*time_step,:6]
       y=normalized_test_data[i*time_step:(i+1)*time_step,6]
       test_x.append(x.tolist())
       test_y.extend(y)
    test_x.append((normalized_test_data[(i+1)*time_step:,:6]).tolist())
    test_y.extend((normalized_test_data[(i+1)*time_step:,6]).tolist())
    return mean,std,test_x,test_y
def get_allt_data(data,time_step=20,test_begin=2923):
    #data_test=data[test_begin:]
    train_data=data[0:2923]
    mean=np.mean(train_data,axis=0)
    std=np.std(train_data,axis=0)
    normalized_all_data=(train_data-mean)/std  #标准化
    size=(len(normalized_all_data)+time_step)//time_step  #有size个sample 
    all_x,all_y=[],[]  
    for i in range(size-1):
       x=normalized_all_data[i*time_step:(i+1)*time_step,:6]
       y=normalized_all_data[i*time_step:(i+1)*time_step,6]
       all_x.append(x.tolist())
       all_y.extend(y)
    all_x.append((normalized_all_data[(i+1)*time_step:,:6]).tolist())
    all_y.extend((normalized_all_data[(i+1)*time_step:,6]).tolist())
    return mean,std,all_x,all_y
def readgasload():
    gasfile=open('gasloadcsv7.csv') 
    datafile=pd.read_csv(gasfile)     
    data=datafile.iloc[:,3:10].values
    return data#取第3-10列
#    i=0
#    traingasload=[]#包括两项，一项为温度（第t天），一项是前一天的燃气量，因此是2600x2矩阵
#    traingasloadlabel=[]#燃气量 2600
#    validationgasload=[]#包括两项，一项为温度（第t天），一项是前一天的燃气量，因此是360x2矩阵
#    validationgasloadlabel=[]#燃气量 360
#    testgasload=[]#包括两项，一项为温度（第t天），一项是前一天的燃气量，因此是150x2矩阵
#    testgasloadlabel=[]#燃气量 150
#    with open('GasLoadsum.txt') as file_object:
#         GasLodSum= file_object.read().splitlines()
#         while i<2600:
#              traingasload.append(GasLodSum[i])
#              i=i+1
#        #print(traingasload)
#         while i<2960:
#              validationgasload.append(GasLodSum[i])
#              i=i+1
#        #print(validationgasload)
#         while i<3110:
#              testgasload.append(GasLodSum[i])
#              i=i+1
#        
#    traingasload,traingasloadlabel=extract_gasload(traingasload,traingasloadlabel)
#    validationgasload,validationgasloadlabel=extract_gasload(validationgasload,validationgasloadlabel)
#    testgasload,testgasloadlabel=extract_gasload(testgasload,testgasloadlabel)
#    return traingasloadlabel,validationgasloadlabel,testgasloadlabel
#def gettraindata(data,time_step):
#    gasdatanumbers = list(map(int, data))
#    normalize_data=(gasdatanumbers-np.mean(gasdatanumbers))/np.std(gasdatanumbers)  #标准化
#    normalize_data=normalize_data[:,np.newaxis]  #增加维度
#    train_x,train_y=[],[]   #训练集
#    for i in range(len(normalize_data)-time_step-1):
#        x=normalize_data[i:i+time_step]
#        y=normalize_data[i+1:i+time_step+1]
#        train_x.append(x.tolist())
#        train_y.append(y.tolist())
#    return train_x,train_y,normalize_data
class PTBModel(object):
    """The PTB model."""
    def __init__(self, is_training, config):
        batch_size = config.batch_size#批量大小60
        self.num_steps = config.num_steps#步数20
        size = config.hidden_size#cell里面三个门网络和一个输入网络的隐含层神经元个数
        self.input_data = tf.placeholder(tf.float32, [None,self.num_steps,config.input_size])#存储输入数据【20,6】
        self.targets = tf.placeholder(tf.float32, [None,self.num_steps,config.output_size])#存储输出数据【20,1】
        weights={
         'in':tf.Variable(tf.random_normal([config.input_size,size])),
         'out':tf.Variable(tf.random_normal([size,1]))
         }
        biases={
        'in':tf.Variable(tf.constant(0.1,shape=[size,])),
        'out':tf.Variable(tf.constant(0.1,shape=[1,]))
         }
        batch_size=tf.shape(self.input_data)[0]
        time_step=tf.shape(self.input_data)[1]
        w_in=weights['in']
        b_in=biases['in']
        inputr=tf.reshape(self.input_data,[-1,config.input_size])  #需要将tensor转成2维进行计算，计算后的结果作为隐藏层的输入
        input_rnn=tf.matmul(inputr,w_in)+b_in
        input_rnn=tf.reshape(input_rnn,[-1,self.num_steps,size])  #将tensor转成3维，作为lstm cell的输入
        lstm_cell=tf.nn.rnn_cell.BasicLSTMCell(size)
        if is_training and config.keep_prob < 1:
            lstm_cell=tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=config.keep_prob)
        cell = tf.contrib.rnn.MultiRNNCell([lstm_cell] * config.num_layers,state_is_tuple=True)
        self.init_state=cell.zero_state(batch_size,dtype=tf.float32)
        output_rnn,final_states=tf.nn.dynamic_rnn(cell, input_rnn,initial_state=self.init_state, dtype=tf.float32)  #output_rnn是记录lstm每个输出节点的结果，final_states是最后一个cell的结果
        output=tf.reshape(output_rnn,[-1,size]) #作为输出层的输入
        w_out=weights['out']
        b_out=biases['out']
        self.pred=tf.matmul(output,w_out)+b_out
        
        self.loss=tf.reduce_mean(tf.square(tf.reshape(self.pred,[-1])-tf.reshape(self.targets, [-1])))
        if not is_training:
            return
        #self.train_op=tf.train.AdamOptimizer(config.learning_rate).minimize(self.loss)
        self._lr = tf.Variable(0.0, trainable=False)
        tvars = tf.trainable_variables()
        grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, tvars), config.max_grad_norm)#截断梯度方法
        optimizer = tf.train.GradientDescentOptimizer(self._lr)#梯度下降方法---阶段梯度的梯度下降法
        self.train_op = optimizer.apply_gradients(zip(grads, tvars))## 定义训练步骤
        self._new_lr = tf.placeholder(tf.float32, shape=[], name="new_learning_rate")
        self._lr_update = tf.assign(self._lr, self._new_lr)
    def assign_lr(self, session, lr_value):
        session.run(self._lr_update, feed_dict={self._new_lr: lr_value})
#def lstm():
#    weights={
#         'in':tf.Variable(tf.random_normal([input_size,rnn_unit])),
#         'out':tf.Variable(tf.random_normal([rnn_unit,1]))
#         }
#        biases={
#        'in':tf.Variable(tf.constant(0.1,shape=[rnn_unit,])),
#        'out':tf.Variable(tf.constant(0.1,shape=[1,]))
#         }     
#    X=tf.placeholder(tf.float32, shape=[None,time_step,input_size])
#    Y=tf.placeholder(tf.float32, shape=[None,time_step,output_size])
#    batch_size=tf.shape(X)[0]
#    time_step=tf.shape(X)[1]
#    w_in=weights['in']
#    b_in=biases['in']  
#    input=tf.reshape(X,[-1,input_size])  #需要将tensor转成2维进行计算，计算后的结果作为隐藏层的输入
#    input_rnn=tf.matmul(input,w_in)+b_in
#    input_rnn=tf.reshape(input_rnn,[-1,time_step,rnn_unit])  #将tensor转成3维，作为lstm cell的输入
#    cell=tf.nn.rnn_cell.BasicLSTMCell(rnn_unit)
#    init_state=cell.zero_state(batch_size,dtype=tf.float32)
#    output_rnn,final_states=tf.nn.dynamic_rnn(cell, input_rnn,initial_state=init_state, dtype=tf.float32)  #output_rnn是记录lstm每个输出节点的结果，final_states是最后一个cell的结果
#    output=tf.reshape(output_rnn,[-1,rnn_unit]) #作为输出层的输入
#    w_out=weights['out']
#    b_out=biases['out']
#    pred=tf.matmul(output,w_out)+b_out
#    loss=tf.reduce_mean(tf.square(tf.reshape(pred,[-1])-tf.reshape(Y, [-1])))
#    train_op=tf.train.AdamOptimizer(lr).minimize(loss)
#    return pred,final_states
#def train_lstm(batch_size=60,time_step=20,train_begin=0,train_end=2923):
    
#    batch_index,train_x,train_y=get_train_data(batch_size,time_step,train_begin,train_end)
    #pred,_=lstm(X)
    #损失函数
    
    #saver=tf.train.Saver(tf.global_variables(),max_to_keep=15)
    #module_file = tf.train.latest_checkpoint()    
#    with tf.Session() as sess:
#        #sess.run(tf.global_variables_initializer())
#        #saver.restore(sess, module_file)
#        #重复训练2000次
#        for i in range(2000):
#            for step in range(len(batch_index)-1):
#                _,loss_=sess.run([train_op,loss],feed_dict={X:train_x[batch_index[step]:batch_index[step+1]],Y:train_y[batch_index[step]:batch_index[step+1]]})
#            print(i,loss_)
#            if i % 200==0:
#                print("保存模型：",saver.save(sess,'stock2.model',global_step=i))
def train_lstm(gasdata,config):
    batch_index,train_x,train_y=get_train_data(gasdata,batch_size=60,time_step=20,train_begin=0,train_end=2923)
    #with tf.name_scope("Train"):
        #with tf.variable_scope("Model", reuse=None):
    train_mode=PTBModel(is_training=True, config=config)
    saver=tf.train.Saver(tf.global_variables(),max_to_keep=15)
    module_file = tf.train.latest_checkpoint("logs/")
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        #saver.restore(sess, module_file)
        #重复训练10000次-训练集
        for i in range(config.max_max_epoch):
            itemd=0
            total_loss=0
            if i%config.max_epoch==0:
                train_mode.assign_lr(sess, config.learning_rate * config.lr_decay)
            for step in range(len(batch_index)-1):
                _,loss_=sess.run([train_mode.train_op,train_mode.loss],feed_dict={train_mode.input_data:train_x[batch_index[step]:batch_index[step+1]],train_mode.targets:train_y[batch_index[step]:batch_index[step+1]]})
                itemd+=1
                total_loss+=loss_
            print(i,total_loss/itemd)
            if i % 200==0:
                checkpoint_file = os.path.join("logs/", 'stock2.model')
                print("保存模型：",saver.save(sess,checkpoint_file,global_step=i))
def prediction(gasdata,config,time_step=20):
    mean,std,test_x,test_y=get_test_data(gasdata,time_step=20,test_begin=2923)
    #print(len(test_x[155]))
    #batch_index,train_x,train_y=get_train_data(gasdata,batch_size=60,time_step=20,train_begin=0,train_end=2923)
    #with tf.name_scope("test"):
        #with tf.variable_scope("Model", reuse=True):
    test_model = PTBModel(is_training=False, config=config)#构建训练模型lstm网络，初始参数用初始化器进行初始化
    saver=tf.train.Saver(tf.global_variables())
    with tf.Session() as sess:
        test_predict=[]
        #test_y=test_y[0:180]
        module_file = tf.train.latest_checkpoint("logs/")
        saver.restore(sess, module_file) 
        for step in range(len(test_x)-1):
            prob=sess.run(test_model.pred,feed_dict={test_model.input_data:[test_x[step]]})   
            predict=prob.reshape((-1))
            test_predict.extend(predict)
        test_y=np.array(test_y)*std[6]+mean[6]
        test_predict=np.array(test_predict)*std[6]+mean[6]
        #acc=evalmape(test_predict, test_y)
        #print(test_y)
       # print(test_predict)
        mape=np.average(np.abs(test_predict-test_y[:len(test_predict)])/test_y[:len(test_predict)]) #acc为测试集偏差
        print(mape)
        #print(test_y)
        #print(len(test_predict))
        plt.figure()
        plt.plot(list(range(len(test_y))), test_y, color='b')
        plt.plot(list(range(len(test_predict))), test_predict, color='r')
        #plt.plot(list(range(len(train_normalize_data), len(train_normalize_data) + len(predict))), predict, color='r')
        plt.show()
        return test_predict
def evalmape(preds, gaps):
    err = abs(gaps-preds)/gaps
    err[(gaps==0)] = 0
    err = np.mean(err)
    return 'error',err  
def main(_):
    gasdata=readgasload()
    config =SmallConfig()#用small类型的lstm网络
    eval_config =SmallConfig()
    eval_config.batch_size = 1
    #print(gasdata)
    #train_lstm(gasdata,config)#0.039
    #test_predict=prediction(gasdata,eval_config,time_step=20)#
    #print(test_predict)
    #np.savetxt("trang.csv", test_predict,fmt='%s')
#    print(len(test_predict))
    #normalized_train_data,batch_index,train_x,train_y=get_train_data(gasdata,batch_size=60,time_step=20,train_begin=0,train_end=2923)
    #print(normalized_train_data)
    #print(normalized_train_data[6].max())
#    mean,std,test_x,test_y=get_test_data(gasdata,time_step=20,test_begin=2923)
#    config =SmallConfig()#用small类型的lstm网络
#    eval_config =SmallConfig()
#    eval_config.batch_size = 1
    #print(len(train_x))
#    print(mean)
#    print(std)
#    with tf.name_scope("Train"):
#        with tf.variable_scope("Model", reuse=None):
#            train_mode=PTBModel(is_training=True, config=config)
##    with tf.name_scope("Valid"):
##        with tf.variable_scope("Model", reuse=True):
##            Valid_model = PTBModel(is_training=False, config=eval_config)#构建训练模型lstm网络，初始参数用初始化器进行初始化
#    with tf.name_scope("test"):
#        with tf.variable_scope("Model", reuse=True):
#            test_model = PTBModel(is_training=False, config=config)#构建训练模型lstm网络，初始参数用初始化器进行初始化
#    saver=tf.train.Saver(tf.global_variables(),max_to_keep=15)
#    with tf.Session() as sess:
#        sess.run(tf.global_variables_initializer())
#        #重复训练10000次-训练集
#        for i in range(config.max_max_epoch):
#            itemd=0
#            total_loss=0
#            if i%30==0:
#                train_mode.assign_lr(sess, config.learning_rate * config.lr_decay)
#            for step in range(len(batch_index)-1):
#                _,loss_=sess.run([train_mode.train_op,train_mode.loss],feed_dict={train_mode.input_data:train_x[batch_index[step]:batch_index[step+1]],train_mode.targets:train_y[batch_index[step]:batch_index[step+1]]})
#                itemd+=1
#                total_loss+=loss_
#            print(i,total_loss/itemd)
#            if i % 200==0:
#                checkpoint_file = os.path.join("logs/", 'stock2.model')
#                print("保存模型：",saver.save(sess,checkpoint_file,global_step=i))
#        test_predict=[]
#        test_y=test_y[0:180]
#        for step in range(len(test_x)-1):
#             prob=sess.run(test_model.pred,feed_dict={test_model.input_data:[test_x[step]]})   
#             predict=prob.reshape((-1))
#             test_predict.extend(predict)
#        test_y=np.array(test_y)*std[2]+mean[2]
#        test_predict=np.array(test_predict)*std[2]+mean[2]
#        acc=np.average(np.abs(test_predict-test_y[:len(test_predict)])/test_y[:len(test_predict)]) #acc为测试集偏差
#        print(acc)
#        #print(test_y)
#        #print(len(test_predict))
#        plt.figure()
#        plt.plot(list(range(len(test_y))), test_y, color='b')
#        plt.plot(list(range(len(test_predict))), test_predict, color='r')
#        #plt.plot(list(range(len(train_normalize_data), len(train_normalize_data) + len(predict))), predict, color='r')
#        plt.show()
#        X=tf.placeholder(tf.float32, shape=[None,10,2])
#        for step in range(len(test_x)-1):
#             X=test_x[step]
#             batch_size=tf.shape(X)[0]
    #print(len(test_x[0]))
         #time_step=tf.shape(X)[1]
#            step=0
#            start=0
#            end=start+config.batch_size
#            total_loss=0
#            while(end<len(train_x)):
#                _,loss_=sess.run([train_mode.train_op,train_mode.loss],feed_dict={train_mode.input_data:train_x[start:end],train_mode.targets:train_y[start:end]})
#                start+=config.batch_size
#                end=start+config.batch_size
#                #每10步保存一次参数
##                if step%10==0:
##                    print(i,step,loss_)
##                   saver.save(sess,'stock.model')
#                step+=1
#                total_loss+=loss_
#            #print()
##            print(len(train_x))
##            print(step)
#            print(i,(total_loss*1.0)/step)
         #验证集训练
          #以折线图表示结果
#        plt.figure()
#        plt.plot(list(range(len(validation_normalize_data))), validation_normalize_data, color='b')
#        print(validation_x)
#        for i in range(10):
#             start=0
#             step=0
#             end=start+60
#             total_loss=0.0
#             while(end<len(test_x)):
#                loss_=sess.run([test_model.loss],feed_dict={test_model.input_data:test_x[start:end],test_model.targets:test_y[start:end]})
#                start+=60
#                end=start+60
#                #每10步保存一次参数
#                if step%1==0:
#                    print(i,step,loss_)
##                   #saver.save(sess,'stock.model')
#                step+=1
#                total_loss+=loss_[0]
#                 print(pred)
#             print(i,loss_)
#             print(i,total_loss/150.0)    
#        prev_seq=train_x[-1]
#        #print(prev_seq)
#        predict=[]
#        #得到之后100个预测结果
#        for i in range(360):
#            next_seq=sess.run(Valid_model.pred,feed_dict={Valid_model.input_data:[prev_seq]})
#            predict.append(next_seq[-1])
#            #每次得到最后一个时间步的预测结果，与之前的数据加在一起，形成新的测试样本
#            prev_seq=np.vstack((prev_seq[1:],next_seq[-1]))
##            print(len(prev_seq))
##            print(next_seq[-1])
##            print(prev_seq[1:])
##        #以折线图表示结果
#        plt.figure()
#        plt.plot(list(range(len(validation_normalize_data))), validation_normalize_data, color='b')
#        plt.plot(list(range(len(predict))), predict, color='r')
#        #plt.plot(list(range(len(train_normalize_data), len(train_normalize_data) + len(predict))), predict, color='r')
#        plt.show()
class SmallConfig(object):
    """Small config."""
    learning_rate =1.0#0.0006#学习率
    max_grad_norm = 5#截断梯度阈值，为了解决梯度爆炸
    num_layers = 3# 隐含层的层数
    num_steps = 20# unrolled 之后的级联cell数
    hidden_size =10# 单个cell中，在对输入进行embedding之后，单个cell的状态及单个cell的输入的维度
    max_epoch = 60#每迭代4次。学习率衰减一次，根据衰减率
    max_max_epoch =1000#迭代次数
    keep_prob = 0.90#dropout率，1为不drouout
    lr_decay = 0.5#学习率递减率
    batch_size =60#批量大小
    input_size=6
    output_size=1
if __name__ == "__main__":
    tf.app.run()   
