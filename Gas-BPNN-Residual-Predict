import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import pandas as pd
import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'

class PTBModel(object):
    """The PTB model."""
    def __init__(self, is_training, config):
        batch_size = config.batch_size#批量大小60
        self.input_data = tf.placeholder(tf.float32, [batch_size,config.input_size])#存储输入数据【20,2】
        self.targets = tf.placeholder(tf.float32, [batch_size,config.output_size])#存储输出数据【20,1】
        
       #tf.nn.relu()
        weights1 = tf.Variable(tf.truncated_normal([config.input_size, config.hidden1_units],stddev=1.0),name='weights1')
        biases1 = tf.Variable(tf.zeros([config.hidden1_units]),name='biases1')
        hidden1 = tf.nn.elu(tf.matmul(self.input_data, weights1) + biases1)
        hidden1 = tf.nn.dropout(hidden1, config.keep_prob)
        
        #backweights = tf.Variable(tf.truncated_normal([config.hidden2_units, config.input_size],stddev=1.0),name='backweights')
        #backbiases = tf.Variable(tf.zeros([config.input_size]),name='backbiases')
        #backhidden = tf.nn.elu(tf.matmul(hidden1, backweights) + backbiases)
       # backhidden = tf.nn.dropout(backhidden, config.keep_prob)
#        weights2 = tf.Variable(tf.truncated_normal([config.hidden1_units, config.hidden2_units],stddev=1.0),name='weights2')
#        biases2 = tf.Variable(tf.zeros([config.hidden2_units]),name='biases2')
#        hidden2 = tf.nn.elu(tf.matmul(hidden1, weights2) + biases2)
#        hidden2 = tf.nn.dropout(hidden2, config.keep_prob)
        
        #backweights2 = tf.Variable(tf.truncated_normal([config.hidden2_units, config.hidden2_units],stddev=1.0),name='backweights2')
        #backbiases2 = tf.Variable(tf.zeros([config.hidden2_units]),name='backbiases2')
        
        #backhidden1 = tf.nn.elu(tf.matmul(hidden2, weights2) + biases2)
        #backhidden1 = tf.nn.dropout(backhidden1, config.keep_prob)
        #backweights3 = tf.Variable(tf.truncated_normal([config.hidden2_units, config.output_size],stddev=1.0 ),name='backweights3')
        #backbiases3 = tf.Variable(tf.zeros([config.output_size]),name='backbiases3')
        #backhidden2 = tf.nn.elu(tf.matmul(backhidden1, weights2) + biases2)
        #backhidden2 = tf.nn.dropout(backhidden2, config.keep_prob)
        #self.back = tf.matmul(hidden1, weights2) + biases2
        
        weights3 = tf.Variable(tf.truncated_normal([config.hidden1_units, config.output_size],stddev=1.0 ),name='weights3')
        biases3 = tf.Variable(tf.zeros([config.output_size]),name='biases3')
        self.logits = tf.matmul(hidden1, weights3) + biases3
        #self.logits=(self.logitsback+self.logits)/2
        #weights4 = tf.Variable(tf.truncated_normal([config.hidden1_units, config.output_size],stddev=1.0 ),name='weights4')
        #biases4 = tf.Variable(tf.zeros([config.output_size]),name='biases4')
       # weights5 = tf.Variable(tf.truncated_normal([config.input_size, config.output_size],stddev=1.0 ),name='weights5')
        #biases5 = tf.Variable(tf.zeros([config.output_size]),name='biases5')
        #self.logits3 = tf.matmul(self.input_data, weights5) + biases5
        #self.logits2 = tf.matmul(hidden1, weights4,) + biases4
        #self.logits=(self.logits2+(self.logits1)+(self.logits3))/3
        #self.logits4=tf.stack([self.logits1,self.logits2,self.logits3],axis=1)
        #self.logits4= tf.reshape(self.logits4,(batch_size,3))
        #print( self.logits)
        #weights6 = tf.Variable(tf.truncated_normal([3, 1],stddev=1.0 ),name='weights6')
        #biases6 = tf.Variable(tf.zeros([1]),name='biases6')
        #self.logits =tf.matmul(self.logits4, weights6) + biases6
        #hidden6 = tf.nn.dropout(hidden6, config.keep_prob)
#        weights7 = tf.Variable(tf.truncated_normal([8, 8],stddev=1.0 ),name='weights7')
#        biases7 = tf.Variable(tf.zeros([8]),name='biases7')
#        hidden7 = tf.nn.relu(tf.matmul(hidden6, weights7) + biases7)
#        hidden7 = tf.nn.dropout(hidden7, config.keep_prob)
#        weights8 = tf.Variable(tf.truncated_normal([8, 1],stddev=1.0 ),name='weights8')
#        biases8 = tf.Variable(tf.zeros([1]),name='biases8')
#        self.logits=tf.matmul(hidden7, weights8,) + biases8
        #self.logits1.extend(self.logits2)
        #self.logits1.extend(self.logits3)
        self.variables_dict = {'weights1': weights1, 'biases1': biases1,'weights3': weights3, 'biases3': biases3}
        self.loss=tf.reduce_mean(tf.square(self.logits-self.targets))
        if not is_training:
            return 
        optimizer = tf.train.AdamOptimizer(0.01)
        self.train_op = optimizer.minimize(self.loss)
       
def readgasload():
    gasfile=open('gasloadcsv9.csv') 
    datafile=pd.read_csv(gasfile)     
    data=datafile.iloc[:,3:8].values
    return data
def get_train_data(data,batch_size=60,train_begin=0,train_end=2740):
    data_train=data[train_begin:train_end]
    normalized_train_data=(data_train-np.mean(data_train,axis=0))/np.std(data_train,axis=0)  #标准化z-score 标准化
    train_x,train_y=[],[]   #训练集x和y初定义
    train_x=normalized_train_data[0:2740,:2]
    train_y=normalized_train_data[0:2740,2,np.newaxis]
    #train_y.reshape(2923,1)
    #train_y.extend(y)
    return train_x,train_y
def get_testv_data(data,test_begin=2740):
    data_test=data[2740:2920]
    train_data=data[0:2740]
    mean=np.mean(train_data,axis=0)
    std=np.std(train_data,axis=0)
    normalized_test_data=(data_test-mean)/std  #标准化
    test_x,test_y=[],[]  
    test_x=normalized_test_data[0:,:2]
    test_y=normalized_test_data[0:,2]
    #print(data_test)
    return mean,std,test_x,test_y
    #data_test=data[2740:2920]
    #train_data=data[0:2920]
    #mean=np.mean(train_data,axis=0)
    #std=np.std(train_data,axis=0)
    #normalized_test_data=(data_test-mean)/std  #标准化
    #test_x,test_y=[],[]  
    #testv_x=data_test[0:,:6]#特征值
    #testv_y=data_test[0:,6]#预测值
    #testlv_y=data_test[0:,7]#实际值
    
   # normalized_testv_x=(testv_x-np.mean(testv_x,axis=0))/np.std(testv_x,axis=0) 
    #return testv_x,testv_y,testlv_y
def get_test_data(data,test_begin=2920):
    data_test=data[2920:]
    train_data=data[0:2740]
    mean=np.mean(train_data,axis=0)
    std=np.std(train_data,axis=0)
    normalized_test_data=(data_test-mean)/std  #标准化
    test_x,test_y=[],[]  
    test_x=normalized_test_data[0:,:2]
    test_y=normalized_test_data[0:,2]
    return mean,std,test_x,test_y
def train_lstm(gasdata,config):
    train_x,train_y=get_train_data(gasdata,batch_size=60,train_begin=0,train_end=2923)
    
    #print(train_x[:,0])
    #train_autocoder(train_x,autocoder)
    #X_train_reconstruct=autocoder.reconstruct(train_x)
    #X_train_reconstruct_org=X_train_reconstruct*np.std(train_x,axis=0)+np.mean(train_x,axis=0)
#    X_train_reconstruct_ori=preprocessor.inverse_transform(X_train_reconstruct)
    #X_train_transform=autocoder.transform(train_x)
    #X_train_transform_org=X_train_transform*np.std(train_x,axis=0)+np.mean(train_x,axis=0)
    #print(train_x[0])
#    X_train_transform_ori=preprocessor.inverse_transform(X_train_transform)
    #plt.figure()
    #plt.plot(list(range(len(train_x[:,0]))), train_x[:,0], color='b')
    #plt.plot(list(range(len(X_train_transform_org[:,0]))), X_train_transform_org[:,0], color='r')
   
    #with tf.name_scope("Train"):
        #with tf.variable_scope("Model", reuse=None):
    train_mode=PTBModel(is_training=True, config=config)
    
    saver=tf.train.Saver(train_mode.variables_dict)
    module_file = tf.train.latest_checkpoint("logs/")
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        #saver.restore(sess, module_file)
        #重复训练10000次-训练集
        for i in range(config.max_max_epoch):
            itemd=0
            total_loss=0
            start_batch=0;
            end_batch=config.batch_size;
#            if i%config.max_epoch==0:
#                train_mode.assign_lr(sess, config.learning_rate * config.lr_decay)
            while end_batch<len(train_x):
                _,loss_=sess.run([train_mode.train_op,train_mode.loss],feed_dict={train_mode.input_data:train_x[start_batch:end_batch],train_mode.targets:train_y[start_batch:end_batch]})
                itemd+=1
                total_loss+=loss_
                start_batch+=config.batch_size 
                end_batch+=config.batch_size
            print(i,total_loss/itemd)
            if i==4800:
                checkpoint_file = os.path.join("logs/", 'stock2.model')
                print("保存模型：",saver.save(sess,checkpoint_file,global_step=i))
def prediction(gasdata,config):
    mean,std,test_x,test_y=get_test_data(gasdata,test_begin=2923)
    #print(test_x)
    #testv_x,testv_y,testlv_y=get_testv_data(gasdata,test_begin=2900)
    #print(mean)
    #with tf.name_scope("test"):
        #with tf.variable_scope("Model", reuse=True):
    #X_test_reconstruct=autocoder.reconstruct(test_x)
    test_model = PTBModel(is_training=False, config=config)#构建训练模型lstm网络，初始参数用初始化器进行初始化
    saver=tf.train.Saver(test_model.variables_dict)
    with tf.Session() as sess:
        test_predict=[]
        test_y=test_y[0:180]
        test_x=test_x[0:180]
        module_file = tf.train.latest_checkpoint("logs/")
        saver.restore(sess, module_file) 
        for step in range(len(test_x)-1):
            prob=sess.run(test_model.logits,feed_dict={test_model.input_data:[test_x[step]]})   
            predict=prob.reshape((-1))
            test_predict.extend(predict)
            
        test_y=np.array(test_y)*std[2]+mean[2]
        test_predict=np.array(test_predict)*std[2]+mean[2]
        #print(test_predict.shape)
        #acc=evalmape(test_predict, test_y)
        #print(test_y)
        return test_predict
#        mape=np.average(np.abs(test_predict-test_y[:len(test_predict)])/test_y[:len(test_predict)]) #acc为测试集偏差
#        return mape
        #print(mape)
        #print(test_y)
        #print(len(test_predict))
        #plt.figure()
        #plt.plot(list(range(len(test_y))), test_y, color='b')
        #plt.plot(list(range(len(test_predict))), test_predict, color='r')
        #plt.plot(list(range(len(train_normalize_data), len(train_normalize_data) + len(predict))), predict, color='r')
        #plt.show()
def evalmape(preds, gaps):
    err = abs(gaps-preds)/gaps
    err[(gaps==0)] = 0
    err = np.mean(err)
    return 'error',err  
def run_training():
    gasdata=readgasload()
    config =SmallConfig()#用small类型的lstm网络
    eval_config =SmallConfig()
    eval_config.batch_size = 1
#    weights1 =tf.truncated_normal([config.input_size, config.hidden1_units],stddev=1.0)
#    biases1 =tf.zeros([config.hidden1_units])
#    with tf.Session() as sess:
#        print(sess.run(biases1))
    #train_x,train_y=get_train_data(gasdata,batch_size=60,train_begin=0,train_end=2923)
    #print(train_y)
    #print(gasdata)KU
    #print(gasdata)
    train_lstm(gasdata,config)#0.039
    test_predict=prediction(gasdata,eval_config)#
#    #print(mape)
    for i in test_predict:
         print(i)
    #print(gasdata.shape())

 
#  data_sets = gasload.read_gasload()
#  
#  with tf.Graph().as_default():
#    
#    gasload_placeholder, gaslodlabels_placeholder = placeholder_inputs(
#        FLAGS.batch_size)
#
#    logits = gt.inference(gasload_placeholder,
#                             FLAGS.hidden1)
#    loss = gt.loss(logits, gaslodlabels_placeholder)
#    train_op = gt.training(loss, FLAGS.learning_rate)
#    #eval_correct = gt.evaluation(logits, gaslodlabels_placeholder)
#    summary_op = tf.summary.merge_all()
#    init = tf.initialize_all_variables()
#    saver = tf.train.Saver()
#    sess = tf.Session()
#    summary_writer = tf.summary.FileWriter("logs/", sess.graph)
#    sess.run(init)
#    for step in xrange(FLAGS.max_steps):
#      start_time = time.time()
#
#      feed_dict = fill_feed_dict(data_sets.train,
#                                 gasload_placeholder,
#                                 gaslodlabels_placeholder)
#      _,loss_value = sess.run([train_op,loss],
#                               feed_dict=feed_dict)
#
#      duration = time.time() - start_time
#      if step % 100== 0:
#        print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))
#        summary_str = sess.run(summary_op, feed_dict=feed_dict)
#        summary_writer.add_summary(summary_str, step)
#        summary_writer.flush()
#        checkpoint_file = os.path.join("logs/", 'checkpoint')
#        saver.save(sess, checkpoint_file, global_step=step)
#     #saver = tf.train.Saver()
#    checkpoint_path = 'C:/Users/Priate/.spyder-py3/logs/checkpoint-4900'
#    reader = tf.train.NewCheckpointReader(checkpoint_path)
#    var = reader.get_variable_to_shape_map()
#    for key in var:
#        if 'weights' in key and  'hidden1' in key:
#            print('tensorname:', key)
#            print(reader.get_tensor(key))
#    b = tf.get_default_graph().get_tensor_by_name("hidden1/biases:0")
#    w = tf.get_default_graph().get_tensor_by_name("hidden1/weights:0")
#    b1 = tf.get_default_graph().get_tensor_by_name("linear/biases:0")
#    w1 = tf.get_default_graph().get_tensor_by_name("linear/weights:0")
#    with tf.Session() as sess:
#           tf.global_variables_initializer().run()
#           print(sess.run(b))
#           print(sess.run(w))
#           print(sess.run(b1))
#           print(sess.run(w1))
#    i=0
#    while i<1:
#          feed_dict = fill_feed_dict(data_sets.test,
#                               gasload_placeholder,
#                               gaslodlabels_placeholder)
#          loss_value = sess.run(logits,feed_dict=feed_dict)
#          if i % 10== 0:
#             print(loss_value)
#             print(feed_dict)
##             #print(tf.losses.mean_squared_error(labels,logits))
#          i=i+1
class SmallConfig(object):
    """Small config."""
    learning_rate =0.01#0.0006#学习率
    num_layers = 2# 隐含层的层数
    hidden1_units =8# 
    hidden2_units =8# 
    max_max_epoch =5000#迭代次数
    batch_size =60#批量大小
    input_size=2
    output_size=1
    keep_prob = 0.98
    
def main(_):
  run_training()


if __name__ == '__main__':
  tf.app.run()
